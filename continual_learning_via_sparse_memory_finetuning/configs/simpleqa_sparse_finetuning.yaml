# Configuration for SimpleQA Document Learning with Sparse Memory Finetuning

experiment:
  name: "simpleqa_sparse_ft_t10k_lr2"
  output_dir: "./outputs/simpleqa_sparse_ft"

model:
  base_model: "../memory/checkpoints/1.3b_1m_keys"
  memory_layer_indices: [12]

sparse_finetuning:
  enabled: true
  top_t: 10000  # Larger top-t for documents (more information content)
  use_idf: true
  background_indices_path: "data/background_indices_dclm_1k.pt"
  num_background_batches: 1000
  idf_smoothing: 1.0

training:
  optimizer: "sgd"
  lr: 2.0
  weight_decay: 0.0
  steps: 1824  # Number of document chunks (100 questions * ~18 chunks/question)
  grad_acc_steps: 1

data:
  task: "simpleqa"
  num_questions: 100
  augmentations_per_chunk: 64  # Batch size
  max_seq_length: 512  # Longer for documents
  chunk_size: 500  # Characters per chunk
  seed: 42

evaluation:
  eval_every: 100
  benchmarks: ["nq", "gsm8k", "hellaswag"]
  max_samples: 1000

logging:
  use_wandb: true
  wandb_project: "continual-learning-paper1"
  log_every: 10
  save_every: 500
